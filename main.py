import requests
import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import email.utils
from urllib.parse import quote

ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
print("âœ… Access Token å‰ 10 ç¢¼ï¼š", ACCESS_TOKEN[:10] if ACCESS_TOKEN else "æœªè¨­å®š")

PREFERRED_SOURCES = ['å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'Ettodayæ–°èé›²', 'å·¥å•†æ™‚å ±ç¶²', 
                     'ä¸­æ™‚æ–°èç¶²', 'ä¸­åœ‹æ™‚å ±', 'å°ç£é›…è™å¥‡æ‘©', 'ç¶“æ¿Ÿæ—¥å ±ç¶²', 'é‰…äº¨ç¶²', 
                     'è¯åˆæ–°èç¶²', 'é¡å‘¨åˆŠç¶²',  'è‡ªç”±è²¡ç¶“', 'ä¸­è¯æ—¥å ±', 'å°ç£æ–°ç”Ÿå ±', 
                     'æ—ºå ±', 'ä¸­åœ‹æ™‚å ±', 'ä¸‰ç«‹æ–°èç¶²',  'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è',
                     'ã€Šç¾ä»£ä¿éšªã€‹é›œèªŒ', 'MoneyDJ', 'é è¦‹é›œèªŒ', 'è‡ªç”±æ™‚å ±', 'Ettodayè²¡ç¶“é›²',
                     'é¡é€±åˆŠMirror Media', 'åŒ¯æµæ–°èç¶²', 'Newtalkæ–°è' , 'å¥‡æ‘©è‚¡å¸‚', 'news.cnyes.com',
                     'ä¸­å¤®ç¤¾', 'æ°‘è¦–æ–°èç¶²', 'é¢¨å‚³åª’', 'CMoney', 'å¤§ç´€å…ƒ']

CATEGORY_KEYWORDS = {
    "æ–°å…‰é‡‘æ§": ["æ–°å…‰é‡‘", "æ–°å…‰äººå£½", "æ–°å£½"],
    "å°æ–°é‡‘æ§": ["å°æ–°é‡‘", "å°æ–°äººå£½"],
    "ä¿éšª": ["ä¿éšª", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª"],
    "é‡‘æ§": ["é‡‘æ§", "é‡‘èæ§è‚¡"],
    "å…¶ä»–": []
}

EXCLUDED_KEYWORDS = ['ä¿éšªå¥—', 'é¿å­•å¥—', 'ä¿éšªå¥—ä½¿ç”¨', 'å¸æ³•ä¿éšª', 'å¿…è¦ä¿éšª']

TW_TZ = timezone(timedelta(hours=8))
today = datetime.now(TW_TZ).date()

# å„²å­˜å·²è™•ç†çš„é€£çµï¼Œé¿å…é‡è¤‡
processed_links = set()

def shorten_url(long_url):
    try:
        encoded_url = quote(long_url, safe='')
        api_url = f"http://tinyurl.com/api-create.php?url={encoded_url}"
        res = requests.get(api_url, timeout=5)
        if res.status_code == 200:
            return res.text.strip()
    except Exception as e:
        print("âš ï¸ çŸ­ç¶²å€å¤±æ•—ï¼š", e)
    return long_url


def classify_news(title):
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(kw in title for kw in keywords):
            return category
    return "å…¶ä»–"


def fetch_news():
    rss_urls = [
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+ä¿éšª+OR+é‡‘æ§+OR+äººå£½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    ]

    classified_news = {cat: [] for cat in CATEGORY_KEYWORDS}

    for rss_url in rss_urls:
        try:
            res = requests.get(rss_url)
            print(f"âœ… ä¾†æº: {rss_url} å›æ‡‰ç‹€æ…‹ï¼š{res.status_code}")

            if res.status_code == 200:
                root = ET.fromstring(res.content)
                items = root.findall(".//item")
                print(f"âœ… å¾ {rss_url} æŠ“åˆ° {len(items)} ç­†æ–°è")

                for item in items:
                    title = item.find('title').text
                    link = item.find('link').text
                    pubDate_str = item.find('pubDate').text
                    source_elem = item.find('source')
                    source_name = source_elem.text if source_elem is not None else "æœªæ¨™ç¤º"

                    pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)
                    pub_date = pub_datetime.date()

                    print(f"ğŸ” æª¢æŸ¥ï¼š{title[:20]}... ä¾†æºï¼š{source_name} ç™¼ä½ˆæ—¥ï¼š{pub_date}")

                    if pub_date != today:
                        continue

                    # æ’é™¤æ•æ„Ÿé—œéµå­—
                    if any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                        print(f"â›” æ’é™¤ï¼š{title[:20]}... å«æœ‰æ’é™¤é—œéµå­—")
                        continue

                    if not any(keyword in source_name or keyword in title for keyword in PREFERRED_SOURCES):
                        continue

                    # æª¢æŸ¥æ˜¯å¦å·²ç¶“è™•ç†éæ­¤é€£çµ
                    if link in processed_links:
                        print(f"â›” æ’é™¤ï¼š{title[:20]}... é‡è¤‡é€£çµ")
                        continue

                    short_link = shorten_url(link)
                    processed_links.add(link)  # æ¨™è¨˜æ­¤é€£çµå·²è™•ç†

                    category = classify_news(title)

                    formatted = f"ğŸ“° {title}\nğŸ“Œ ä¾†æºï¼š{source_name}\nğŸ”— {short_link}"
                    classified_news[category].append(formatted)

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•å¾ {rss_url} æŠ“å–æ–°èï¼š{e}")

    news_text = f"ğŸ“… ä»Šæ—¥æ—¥æœŸï¼š{today.strftime('%Y-%m-%d')}\n\n"
    for cat in ["æ–°å…‰é‡‘æ§", "å°æ–°é‡‘æ§","ä¿éšª", "é‡‘æ§", "å…¶ä»–"]:
        if classified_news[cat]:
            news_text += f"ğŸ“‚ã€{cat}ã€‘\n"
            for idx, item in enumerate(classified_news[cat], 1):
                news_text += f"{idx}. {item}\n\n"

    news_text += "ğŸ“ æœ¬æ–°èæ•´ç†è‡ª Google News RSSï¼Œé€£çµå·²è½‰ç‚ºçŸ­ç¶²å€ã€‚"
    print("âœ… ä»Šæ—¥æ–°èå…§å®¹ï¼š\n", news_text)
    return news_text.strip()


def broadcast_message(message):
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }
    data = {
        "messages": [{
            "type": "text",
            "text": message
        }]
    }

    print("âœ… å³å°‡ç™¼é€çš„è³‡æ–™ï¼š")
    print(data)

    res = requests.post(url, headers=headers, json=data)
    print(f"ğŸ“¤ LINE å›å‚³ç‹€æ…‹ç¢¼ï¼š{res.status_code}")
    print("ğŸ“¤ LINE å›å‚³å…§å®¹ï¼š", res.text)


if __name__ == "__main__":
    news = fetch_news()
    if news:
        broadcast_message("ã€æ¥­ä¼éƒ¨ ä»Šæ—¥é‡é»æ–°èæ•´ç†ã€‘\n\n" + news)
    else:
        print("âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°èï¼Œä¸ç™¼é€ã€‚")

