import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import email.utils
from urllib.parse import quote
import requests
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
import hashlib
import json
import sqlite3
from pathlib import Path
import time
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('news_bot.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# âœ… åˆå§‹åŒ–èªæ„æ¨¡å‹ (å¯é¸)
try:
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    logger.info("âœ… AI æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    USE_AI_MODEL = True
except Exception as e:
    logger.warning(f"âš ï¸ AI æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå°‡ä½¿ç”¨æ–‡å­—æ¯”å°: {e}")
    model = None
    USE_AI_MODEL = False

# âœ… å¤§å¹…æ”¾å¯¬å»é‡åƒæ•¸
SIMILARITY_THRESHOLD = 0.95       # èªæ„ç›¸ä¼¼åº¦é–€æª» (æé«˜åˆ°éå¸¸åš´æ ¼)
TEXT_SIMILARITY_THRESHOLD = 0.90   # æ–‡å­—ç›¸ä¼¼åº¦é–€æª» (æé«˜åˆ°éå¸¸åš´æ ¼)
URL_SIMILARITY_THRESHOLD = 0.95    # URL ç›¸ä¼¼åº¦é–€æª» (æé«˜åˆ°éå¸¸åš´æ ¼)
ENABLE_DEDUP = True                # å¯ä»¥é—œé–‰å»é‡åŠŸèƒ½é€²è¡Œæ¸¬è©¦

ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
if ACCESS_TOKEN:
    logger.info(f"âœ… Access Token å·²è¨­å®šï¼Œå‰10ç¢¼ï¼š{ACCESS_TOKEN[:10]}")
else:
    logger.error("âŒ ACCESS_TOKEN ç’°å¢ƒè®Šæ•¸æœªè¨­å®š")

# âœ… æ”¾å¯¬é—œéµå­—æ¯”å°ï¼Œå¢åŠ æ›´å¤šé—œéµå­—
CATEGORY_KEYWORDS = {
    "æ–°å…‰é‡‘æ§": [
        "æ–°å…‰é‡‘", "æ–°å…‰äººå£½", "æ–°å£½", "å³æ±é€²", "æ–°å…‰é‡‘æ§", "æ–°å…‰ä¿éšª",
        "æ–°å…‰éŠ€è¡Œ", "æ–°å…‰æŠ•ä¿¡", "æ–°å…‰è­‰åˆ¸", "Shin Kong"
    ],
    "å°æ–°é‡‘æ§": [
        "å°æ–°é‡‘", "å°æ–°äººå£½", "å°æ–°å£½", "å³æ±äº®", "å°æ–°é‡‘æ§", "å°æ–°ä¿éšª", 
        "å°æ–°éŠ€è¡Œ", "å°æ–°è­‰åˆ¸", "å°æ–°æŠ•ä¿¡", "Taishin"
    ],
    "é‡‘æ§": [
        "é‡‘æ§", "é‡‘èæ§è‚¡", "ä¸­ä¿¡é‡‘", "ç‰å±±é‡‘", "æ°¸è±é‡‘", "åœ‹æ³°é‡‘", "å¯Œé‚¦é‡‘", 
        "å°ç£é‡‘", "ç¬¬ä¸€é‡‘", "å…ƒå¤§é‡‘", "é–‹ç™¼é‡‘", "å…†è±é‡‘", "è¯å—é‡‘", "å½°éŠ€é‡‘",
        "é‡‘èæ¥­", "éŠ€è¡Œæ¥­", "è­‰åˆ¸æ¥­"
    ],
    "ä¿éšª": [
        "ä¿éšª", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª", "äººå£½", "ç”¢éšª", "è»Šéšª", "ä¿å–®",
        "ç†è³ ", "æŠ•ä¿", "ä¿è²»", "ä¿éšªæ¥­", "å†ä¿", "å¾®å‹ä¿éšª", "æ•¸ä½ä¿éšª"
    ],
    "å…¶ä»–": []
}

# âœ… å¤§å¹…æ¸›å°‘æ’é™¤é—œéµå­—ï¼Œåªæ’é™¤æ˜é¡¯ç„¡é—œçš„
EXCLUDED_KEYWORDS = [
    'ä¿éšªå¥—', 'é¿å­•å¥—', 'ä¿éšªå¥—ä½¿ç”¨'
    # ç§»é™¤å…¶ä»–æ’é™¤æ¢ä»¶ï¼Œè®“æ›´å¤šæ–°èé€šé
]

TW_TZ = timezone(timedelta(hours=8))
now = datetime.now(TW_TZ)
today = now.date()

def init_database():
    """åˆå§‹åŒ– SQLite è³‡æ–™åº«"""
    try:
        db_path = Path('news_cache.db')
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS processed_news (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                normalized_title TEXT NOT NULL,
                link TEXT NOT NULL,
                source TEXT,
                category TEXT,
                pub_date TEXT,
                processed_at TEXT NOT NULL,
                title_hash TEXT NOT NULL,
                url_hash TEXT
            )
        ''')
        
        # å»ºç«‹ç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_title_hash ON processed_news(title_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_processed_at ON processed_news(processed_at)')
        
        # åªæ¸…ç†è¶…é3å¤©çš„è¨˜éŒ„ (ç¸®çŸ­æ¸…ç†é€±æœŸ)
        days_ago = (now - timedelta(days=3)).isoformat()
        cursor.execute('DELETE FROM processed_news WHERE processed_at < ?', (days_ago,))
        deleted_count = cursor.rowcount
        
        conn.commit()
        logger.info(f"âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆï¼Œæ¸…ç†äº† {deleted_count} ç­†èˆŠè¨˜éŒ„")
        return conn
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

def normalize_title(title):
    """ç°¡åŒ–çš„æ¨™é¡Œæ­£è¦åŒ– - ä¿ç•™æ›´å¤šåŸå§‹è³‡è¨Š"""
    if not title:
        return ""
    
    # åªåšæœ€åŸºæœ¬çš„æ¸…ç†
    title = re.sub(r'<[^>]+>', '', title)        # ç§»é™¤ HTML æ¨™ç±¤
    title = re.sub(r'\s+', ' ', title)           # åˆä½µå¤šé¤˜ç©ºç™½
    title = title.strip()
    
    # ä¸è½‰æ›å¤§å°å¯«ï¼Œä¿ç•™åŸå§‹æ ¼å¼
    return title

def calculate_simple_similarity(text1, text2):
    """ç°¡åŒ–çš„æ–‡å­—ç›¸ä¼¼åº¦è¨ˆç®—"""
    if not text1 or not text2:
        return 0
    
    # ç°¡å–®çš„é—œéµå­—é‡ç–Šæ¯”è¼ƒ
    words1 = set(text1.split())
    words2 = set(text2.split())
    
    if not words1 or not words2:
        return 0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    return len(intersection) / len(union) if union else 0

def generate_hash(text):
    """ç”Ÿæˆæ–‡å­—é›œæ¹Š"""
    if not text:
        return ""
    return hashlib.md5(text.encode('utf-8')).hexdigest()[:12]

def is_duplicate_news(title, link, conn):
    """å¤§å¹…æ”¾å¯¬çš„é‡è¤‡æª¢æ¸¬"""
    if not ENABLE_DEDUP or not conn:
        return False
    
    try:
        normalized = normalize_title(title)
        title_hash = generate_hash(normalized)
        
        cursor = conn.cursor()
        
        # åªæª¢æŸ¥å®Œå…¨ç›¸åŒçš„æ¨™é¡Œé›œæ¹Š
        cursor.execute('SELECT title FROM processed_news WHERE title_hash = ?', (title_hash,))
        if cursor.fetchone():
            logger.info(f"ğŸ”„ ç™¼ç¾å®Œå…¨ç›¸åŒæ¨™é¡Œ: {title[:50]}...")
            return True
        
        # åªæª¢æŸ¥å®Œå…¨ç›¸åŒçš„é€£çµ
        cursor.execute('SELECT title FROM processed_news WHERE link = ?', (link,))
        if cursor.fetchone():
            logger.info(f"ğŸ”„ ç™¼ç¾ç›¸åŒé€£çµ: {title[:50]}...")
            return True
        
        return False
        
    except Exception as e:
        logger.error(f"âŒ é‡è¤‡æª¢æ¸¬å¤±æ•—: {e}")
        return False

def save_processed_news(title, link, source, category, pub_date, conn):
    """å„²å­˜å·²è™•ç†çš„æ–°è"""
    if not conn:
        return
    
    try:
        normalized = normalize_title(title)
        title_hash = generate_hash(normalized)
        url_hash = generate_hash(link)
        
        cursor = conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO processed_news 
            (id, title, normalized_title, link, source, category, pub_date, processed_at, title_hash, url_hash)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            f"{title_hash}_{int(now.timestamp())}",
            title, normalized, link, source, category, 
            pub_date, now.isoformat(), title_hash, url_hash
        ))
        conn.commit()
    except Exception as e:
        logger.error(f"âŒ å„²å­˜æ–°èå¤±æ•—: {e}")

def create_clean_url(long_url):
    """å»ºç«‹é¿å… LINE é è¦½çš„ç¶²å€"""
    try:
        # ä½¿ç”¨ TinyURL
        encoded_url = quote(long_url, safe='')
        api_url = f"http://tinyurl.com/api-create.php?url={encoded_url}"
        res = requests.get(api_url, timeout=8)
        
        if res.status_code == 200 and res.text.startswith('http'):
            short_url = res.text.strip()
            return f"{short_url}?t={int(time.time())}"
        
    except Exception as e:
        logger.warning(f"âš ï¸ çŸ­ç¶²å€å¤±æ•—: {e}")
    
    # å‚™ç”¨æ–¹æ¡ˆ
    separator = '&' if '?' in long_url else '?'
    return f"{long_url}{separator}t={int(time.time())}"

def classify_news(title):
    """æ”¾å¯¬çš„æ–°èåˆ†é¡"""
    title_lower = title.lower()
    
    # ä¾åºæª¢æŸ¥åˆ†é¡
    for category, keywords in CATEGORY_KEYWORDS.items():
        if category == "å…¶ä»–":
            continue
        for keyword in keywords:
            if keyword.lower() in title_lower:
                logger.info(f"ğŸ“‚ åˆ†é¡ç‚º [{category}]: {keyword} in {title[:50]}")
                return category
    
    logger.info(f"ğŸ“‚ åˆ†é¡ç‚º [å…¶ä»–]: {title[:50]}")
    return "å…¶ä»–"

def is_taiwan_news(source_name, link, title):
    """å¤§å¹…æ”¾å¯¬å°ç£æ–°èåˆ¤æ–·"""
    # å¤§å¹…æ“´å±•å°ç£æ–°èä¾†æº
    taiwan_sources = [
        'å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'ä¸‰ç«‹æ–°èç¶²', 'è‡ªç”±æ™‚å ±', 'è¯åˆæ–°èç¶²',
        'é¡é€±åˆŠ', 'å°ç£é›…è™', 'é‰…äº¨ç¶²', 'ä¸­æ™‚æ–°èç¶²', 'Ettodayæ–°èé›²', 'ETtoday',
        'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ç¾ä»£ä¿éšª', 'é è¦‹é›œèªŒ', 'å•†æ¥­å‘¨åˆŠ', 'ä»Šå‘¨åˆŠ',
        'MoneyDJ', 'é‰…äº¨', 'Anue', 'ç†è²¡ç¶²', 'ç¶“æ¿Ÿé€š', 'å¡å„ªæ–°èç¶²', 'Yahoo',
        'è˜‹æœæ–°èç¶²', 'ä¸­å¤®ç¤¾', 'NOWnews', 'é¢¨å‚³åª’', 'æ–°é ­æ®¼', 'Newtalk',
        'è²¡è¨Š', 'Smart', 'é‰…äº¨ç¶²', 'å·¥å•†', 'ç¶“æ¿Ÿ'
    ]
    
    # æª¢æŸ¥ä¾†æº (ç§»é™¤é¦™æ¸¯æ’é™¤æ¢ä»¶ï¼Œå…ˆæ”¾å¯¬æ¸¬è©¦)
    if any(taiwan_source in source_name for taiwan_source in taiwan_sources):
        logger.info(f"âœ… å°ç£æ–°èä¾†æº: {source_name}")
        return True
    
    # æª¢æŸ¥ç¶²åŸŸ
    taiwan_domains = ['.tw', 'yahoo.com', 'google.com', 'ettoday', 'chinatimes', 'udn.com']
    if any(domain in link for domain in taiwan_domains):
        logger.info(f"âœ… å°ç£ç¶²åŸŸ: {link}")
        return True
    
    # å¤§å¹…æ”¾å¯¬ï¼Œå¹¾ä¹æ‰€æœ‰æ–°èéƒ½é€šé
    logger.info(f"âœ… é è¨­é€šé: {source_name}")
    return True

def is_semantically_similar(title, processed_embeddings):
    """èªæ„ç›¸ä¼¼åº¦æª¢æ¸¬ (å¯é¸)"""
    if not USE_AI_MODEL or not model or not processed_embeddings or not ENABLE_DEDUP:
        return False
    
    try:
        norm_title = normalize_title(title)
        if not norm_title:
            return False
        
        current_embedding = model.encode([norm_title])
        similarities = cosine_similarity(current_embedding, processed_embeddings)[0]
        max_similarity = np.max(similarities)
        
        if max_similarity >= SIMILARITY_THRESHOLD:
            logger.info(f"ğŸ”„ ç™¼ç¾èªæ„ç›¸ä¼¼æ–°è ({max_similarity:.3f}): {title[:50]}...")
            return True
        
        return False
        
    except Exception as e:
        logger.error(f"âŒ èªæ„ç›¸ä¼¼åº¦æª¢æ¸¬å¤±æ•—: {e}")
        return False

def fetch_news():
    """ä¸»è¦æ–°èæŠ“å–å‡½æ•¸ - æ”¾å¯¬æ‰€æœ‰æ¢ä»¶"""
    logger.info("ğŸš€ é–‹å§‹æŠ“å–æ–°è...")
    
    conn = init_database()
    
    # ç°¡åŒ– RSS URLsï¼Œç§»é™¤æ™‚é–“é™åˆ¶
    rss_urls = [
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å£½éšª+OR+é‡‘æ§+OR+äººå£½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å£½éšª+OR+å¥åº·éšª+OR+æ„å¤–éšª+OR+äººå£½+OR+ä¿éšª&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=é‡‘æ§+OR+é‡‘èæ§è‚¡+OR+éŠ€è¡Œ&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    ]

    classified_news = {cat: [] for cat in CATEGORY_KEYWORDS}
    processed_embeddings = []
    stats = {
        'total_fetched': 0,
        'processed': 0,
        'duplicates': 0,
        'filtered_out': 0,
        'by_category': {cat: 0 for cat in CATEGORY_KEYWORDS}
    }

    # è¼‰å…¥æœ€è¿‘çš„æ–°è (ç¸®çŸ­æ™‚é–“ç¯„åœ)
    if USE_AI_MODEL and conn:
        try:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT normalized_title FROM processed_news 
                WHERE processed_at > ? 
                LIMIT 100
            ''', ((now - timedelta(hours=24)).isoformat(),))
            
            recent_titles = [row[0] for row in cursor.fetchall()]
            
            if recent_titles and model:
                logger.info(f"âœ… è¼‰å…¥ {len(recent_titles)} å€‹æœ€è¿‘æ¨™é¡Œç”¨æ–¼æ¯”è¼ƒ")
                processed_embeddings = model.encode(recent_titles)
                
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æ­·å²æ¨™é¡Œå¤±æ•—: {e}")

    # è™•ç†æ¯å€‹ RSS ä¾†æº
    for i, rss_url in enumerate(rss_urls, 1):
        try:
            logger.info(f"ğŸ“¡ è™•ç† RSS ä¾†æº {i}/{len(rss_urls)}")
            
            res = requests.get(rss_url, timeout=15, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            if res.status_code != 200:
                logger.warning(f"âš ï¸ RSS å›æ‡‰ç•°å¸¸: {res.status_code}")
                continue

            root = ET.fromstring(res.content)
            items = root.findall(".//item")
            stats['total_fetched'] += len(items)
            
            logger.info(f"âœ… æŠ“åˆ° {len(items)} ç­†æ–°è")

            for item in items:
                try:
                    # è§£ææ–°èé …ç›®
                    title_elem = item.find('title')
                    link_elem = item.find('link')
                    pubDate_elem = item.find('pubDate')
                    
                    if not all([title_elem, link_elem, pubDate_elem]):
                        continue

                    title = title_elem.text.strip()
                    link = link_elem.text.strip()
                    pubDate_str = pubDate_elem.text.strip()
                    
                    if not title or title.startswith("Google ãƒ‹ãƒ¥ãƒ¼ã‚¹"):
                        continue

                    source_elem = item.find('source')
                    source_name = source_elem.text.strip() if source_elem is not None else "æœªæ¨™ç¤º"
                    
                    # è§£æç™¼å¸ƒæ™‚é–“
                    try:
                        pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)
                    except:
                        # å¦‚æœæ™‚é–“è§£æå¤±æ•—ï¼Œä½¿ç”¨ç•¶å‰æ™‚é–“
                        pub_datetime = now
                        logger.warning(f"âš ï¸ æ™‚é–“è§£æå¤±æ•—ï¼Œä½¿ç”¨ç•¶å‰æ™‚é–“: {title[:30]}")

                    logger.info(f"ğŸ“ è™•ç†æ–°è: {title[:50]}... | ä¾†æº: {source_name}")

                    # æ”¾å¯¬åŸºæœ¬éæ¿¾æ¢ä»¶
                    
                    # 1. æ™‚é–“éæ¿¾ - å»¶é•·åˆ°48å°æ™‚
                    if now - pub_datetime > timedelta(hours=48):
                        stats['filtered_out'] += 1
                        logger.info(f"â° æ™‚é–“éæ¿¾: {title[:30]}...")
                        continue
                    
                    # 2. æ’é™¤é—œéµå­—æª¢æŸ¥ - åªæ’é™¤æ¥µå°‘æ•¸
                    if any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                        stats['filtered_out'] += 1
                        logger.info(f"ğŸš« é—œéµå­—éæ¿¾: {title[:30]}...")
                        continue
                    
                    # 3. å°ç£æ–°èæª¢æŸ¥ - å¤§å¹…æ”¾å¯¬
                    if not is_taiwan_news(source_name, link, title):
                        stats['filtered_out'] += 1
                        logger.info(f"ğŸŒ åœ°å€éæ¿¾: {title[:30]}...")
                        continue

                    # 4. é‡è¤‡æª¢æ¸¬ - å¯é¸
                    if is_duplicate_news(title, link, conn):
                        stats['duplicates'] += 1
                        continue
                    
                    if is_semantically_similar(title, processed_embeddings):
                        stats['duplicates'] += 1
                        continue

                    # æ–°èé€šéæ‰€æœ‰æª¢æŸ¥
                    clean_url = create_clean_url(link)
                    category = classify_news(title)
                    
                    # æ ¼å¼åŒ–æ–°è
                    formatted = f"ğŸ“° {title}\nğŸ“Œ {source_name}\nğŸ”— è©³æƒ…: {clean_url}"
                    classified_news[category].append(formatted)
                    stats['by_category'][category] += 1
                    
                    # å„²å­˜åˆ°è³‡æ–™åº«
                    if conn:
                        save_processed_news(title, link, source_name, category, 
                                          pub_datetime.isoformat(), conn)
                    
                    # æ›´æ–°èªæ„æ¯”è¼ƒåŸºæº–
                    if USE_AI_MODEL and model and len(processed_embeddings) > 0:
                        try:
                            new_embedding = model.encode([normalize_title(title)])
                            processed_embeddings = np.vstack([processed_embeddings, new_embedding])
                        except:
                            pass
                    
                    stats['processed'] += 1
                    logger.info(f"âœ… æˆåŠŸè™•ç†: [{category}] {title[:30]}...")
                    
                except Exception as e:
                    logger.error(f"âŒ è™•ç†å–®ç­†æ–°èå¤±æ•—: {e}")
                    continue

        except Exception as e:
            logger.error(f"âŒ è™•ç† RSS ä¾†æºå¤±æ•—: {e}")
            continue

    if conn:
        conn.close()
    
    # è¼¸å‡ºè©³ç´°çµ±è¨ˆ
    logger.info(f"""
ğŸ“Š æ–°èè™•ç†çµ±è¨ˆ:
   ç¸½æŠ“å–: {stats['total_fetched']} ç­†
   æˆåŠŸè™•ç†: {stats['processed']} ç­†
   é‡è¤‡éæ¿¾: {stats['duplicates']} ç­†
   å…¶ä»–éæ¿¾: {stats['filtered_out']} ç­†
   
   åˆ†é¡çµ±è¨ˆ:
   ğŸ“‚ æ–°å…‰é‡‘æ§: {stats['by_category']['æ–°å…‰é‡‘æ§']} ç­†
   ğŸ“‚ å°æ–°é‡‘æ§: {stats['by_category']['å°æ–°é‡‘æ§']} ç­†
   ğŸ“‚ é‡‘æ§: {stats['by_category']['é‡‘æ§']} ç­†
   ğŸ“‚ ä¿éšª: {stats['by_category']['ä¿éšª']} ç­†
   ğŸ“‚ å…¶ä»–: {stats['by_category']['å…¶ä»–']} ç­†
""")
    
    return classified_news

def send_message_by_category(news_by_category):
    """åˆ†é¡ç™¼é€ LINE è¨Šæ¯"""
    if not ACCESS_TOKEN:
        logger.error("âŒ ACCESS_TOKEN æœªè¨­å®šï¼Œç„¡æ³•ç™¼é€è¨Šæ¯")
        return
    
    max_length = 3800
    successful_sends = 0
    total_news = sum(len(msgs) for msgs in news_by_category.values())
    
    logger.info(f"ğŸ“¨ æº–å‚™ç™¼é€ {total_news} å‰‡æ–°è")
    
    for category, messages in news_by_category.items():
        if not messages:
            continue
            
        try:
            logger.info(f"ğŸ“¤ ç™¼é€åˆ†é¡ [{category}]: {len(messages)} å‰‡æ–°è")
            
            # å»ºç«‹åˆ†é¡æ¨™é¡Œ
            title = f"ã€{today} {category} æ–°èã€‘å…± {len(messages)} å‰‡"
            divider = "=" * 35
            content = f"\n{divider}\n".join(messages)
            full_message = f"{title}\n{divider}\n{content}"
            
            # è™•ç†è¶…é•·è¨Šæ¯
            if len(full_message) <= max_length:
                if broadcast_message(full_message):
                    successful_sends += 1
            else:
                # åˆ†æ®µç™¼é€
                segments = []
                current_segment = f"{title}\n{divider}\n"
                
                for msg in messages:
                    if len(current_segment + msg + f"\n{divider}\n") <= max_length:
                        current_segment += msg + f"\n{divider}\n"
                    else:
                        segments.append(current_segment.rstrip(f"\n{divider}\n"))
                        current_segment = f"{title} (çºŒ)\n{divider}\n{msg}\n{divider}\n"
                
                if current_segment.strip():
                    segments.append(current_segment.rstrip(f"\n{divider}\n"))
                
                for i, segment in enumerate(segments):
                    if broadcast_message(segment):
                        successful_sends += 1
                    time.sleep(1)  # é¿å…ç™¼é€éå¿«
                        
        except Exception as e:
            logger.error(f"âŒ ç™¼é€ {category} åˆ†é¡å¤±æ•—: {e}")

    # ç™¼é€ç„¡æ–°èé€šçŸ¥
    empty_categories = [cat for cat, msgs in news_by_category.items() if not msgs]
    if empty_categories:
        title = f"ã€{today} ç„¡æ–°èåˆ†é¡ã€‘"
        content = "\n".join(f"ğŸ“‚ {cat}: ç„¡ç›¸é—œæ–°è" for cat in empty_categories)
        broadcast_message(f"{title}\n{content}")
        
    logger.info(f"âœ… æˆåŠŸç™¼é€ {successful_sends} å€‹è¨Šæ¯æ®µè½")

def broadcast_message(message):
    """ç™¼é€ LINE å»£æ’­è¨Šæ¯"""
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }

    data = {
        "messages": [{
            "type": "text",
            "text": message
        }]
    }

    try:
        logger.info(f"ğŸ“¤ ç™¼é€è¨Šæ¯ ({len(message)} å­—å…ƒ)")
        res = requests.post(url, headers=headers, json=data, timeout=15)
        
        if res.status_code == 200:
            logger.info("âœ… è¨Šæ¯ç™¼é€æˆåŠŸ")
            return True
        else:
            logger.error(f"âŒ ç™¼é€å¤±æ•— - ç‹€æ…‹ç¢¼: {res.status_code}, å›æ‡‰: {res.text}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ç™¼é€è¨Šæ¯ç•°å¸¸: {e}")
        return False

if __name__ == "__main__":
    start_time = time.time()
    logger.info(f"ğŸš€ æ–°èçˆ¬å–ç³»çµ±å•Ÿå‹• (æ”¾å¯¬æ¨¡å¼) - {now}")
    
    # æ¸¬è©¦æ¨¡å¼ï¼šå¯ä»¥é—œé–‰å»é‡åŠŸèƒ½
    if os.getenv('DISABLE_DEDUP') == 'true':
        ENABLE_DEDUP = False
        logger.info("ğŸ”“ å»é‡åŠŸèƒ½å·²é—œé–‰ (æ¸¬è©¦æ¨¡å¼)")
    
    try:
        news = fetch_news()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°è
        total_news = sum(len(msgs) for msgs in news.values())
        
        if total_news > 0:
            logger.info(f"ğŸ“¨ æº–å‚™ç™¼é€ {total_news} å‰‡æ–°è")
            send_message_by_category(news)
        else:
            logger.warning("âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°è")
            # ç™¼é€èª¿è©¦è¨Šæ¯
            if ACCESS_TOKEN:
                debug_msg = f"ã€{today} ç³»çµ±é€šçŸ¥ã€‘\næ–°èçˆ¬å–å®Œæˆï¼Œä½†æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°è\nè«‹æª¢æŸ¥é—œéµå­—å’Œéæ¿¾æ¢ä»¶"
                broadcast_message(debug_msg)
        
        elapsed = time.time() - start_time
        logger.info(f"âœ… ç³»çµ±åŸ·è¡Œå®Œæˆï¼Œè€—æ™‚ {elapsed:.1f} ç§’")
        
    except Exception as e:
        logger.error(f"âŒ ç³»çµ±åŸ·è¡Œå¤±æ•—: {e}")
        # ç™¼é€éŒ¯èª¤é€šçŸ¥
        if ACCESS_TOKEN:
            error_msg = f"ã€ç³»çµ±éŒ¯èª¤ã€‘\næ–°èçˆ¬å–ç³»çµ±åŸ·è¡Œç•°å¸¸\néŒ¯èª¤: {str(e)[:100]}"
            broadcast_message(error_msg)



