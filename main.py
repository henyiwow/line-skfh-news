import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import email.utils
from urllib.parse import quote
import requests
from bs4 import BeautifulSoup

# è¨­å®š ACCESS_TOKEN
ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
print("âœ… Access Token å‰ 10 ç¢¼ï¼š", ACCESS_TOKEN[:10] if ACCESS_TOKEN else "æœªè¨­å®š")

# é è¨­ä¾†æº
PREFERRED_SOURCES = ['å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'Ettodayæ–°èé›²', 'å·¥å•†æ™‚å ±ç¶²',
                     'ä¸­æ™‚æ–°èç¶²', 'å°ç£é›…è™å¥‡æ‘©', 'ç¶“æ¿Ÿæ—¥å ±ç¶²', 'é‰…äº¨ç¶²', 'è¯åˆæ–°èç¶²',
                     'é¡å‘¨åˆŠç¶²', 'è‡ªç”±è²¡ç¶“', 'ä¸­è¯æ—¥å ±', 'å°ç£æ–°ç”Ÿå ±', 'æ—ºå ±', 'ä¸‰ç«‹æ–°èç¶²',
                     'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ã€Šç¾ä»£ä¿éšªã€‹é›œèªŒ', 'MoneyDJ', 'é è¦‹é›œèªŒ',
                     'è‡ªç”±æ™‚å ±', 'Ettodayè²¡ç¶“é›²', 'é¡é€±åˆŠMirror Media', 'åŒ¯æµæ–°èç¶²',
                     'Newtalkæ–°è', 'å¥‡æ‘©è‚¡å¸‚', 'news.cnyes.com', 'ä¸­å¤®ç¤¾', 'æ°‘è¦–æ–°èç¶²',
                     'é¢¨å‚³åª’', 'CMoney', 'å¤§ç´€å…ƒ']

# åˆ†é¡é—œéµå­—
CATEGORY_KEYWORDS = {
    "æ–°å…‰é‡‘æ§": ["æ–°å…‰é‡‘", "æ–°å…‰äººå£½", "æ–°å£½", "å³æ±é€²"],
    "å°æ–°é‡‘æ§": ["å°æ–°é‡‘", "å°æ–°äººå£½", "å°æ–°å£½", "å³æ±äº®"],
    "ä¿éšª": ["ä¿éšª", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª", "äººå£½"],
    "é‡‘æ§": ["é‡‘æ§", "é‡‘èæ§è‚¡", "ä¸­ä¿¡é‡‘", "ç‰å±±é‡‘", "æ°¸è±é‡‘", "åœ‹æ³°é‡‘", "å¯Œé‚¦é‡‘", "å°ç£é‡‘"],
    "å…¶ä»–": []
}

# æ’é™¤é—œéµå­—
EXCLUDED_KEYWORDS = ['ä¿éšªå¥—', 'é¿å­•å¥—', 'ä¿éšªå¥—ä½¿ç”¨', 'å¤ªé™½äººå£½', 'å¤§è¥¿éƒ¨äººå£½', 'ç¾åœ‹æµ·å²¸ä¿éšª']

# å°ç£æ™‚å€è¨­å®š
TW_TZ = timezone(timedelta(hours=8))
today = datetime.now(TW_TZ).date()

# ç”ŸæˆçŸ­ç¶²å€
def shorten_url(long_url):
    try:
        encoded_url = quote(long_url, safe='')
        api_url = f"http://tinyurl.com/api-create.php?url={encoded_url}"
        res = requests.get(api_url, timeout=5)
        if res.status_code == 200:
            return res.text.strip()
    except Exception as e:
        print("âš ï¸ çŸ­ç¶²å€å¤±æ•—ï¼š", e)
    return long_url

# åˆ¤æ–·æ˜¯å¦ç‚ºå°ç£æ–°è
def is_taiwan_news(source_name, link):
    taiwan_sources = ['å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'ä¸‰ç«‹æ–°èç¶²', 'è‡ªç”±æ™‚å ±', 'è¯åˆæ–°èç¶²', 'é¡é€±åˆŠ', 'å°ç£é›…è™', 'é‰…äº¨ç¶²', 'ä¸­æ™‚æ–°èç¶²','Ettodayæ–°èé›²',
                      'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ã€Šç¾ä»£ä¿éšªã€‹é›œèªŒ','é è¦‹é›œèªŒ']
    return any(src in source_name for src in taiwan_sources) or '.tw' in link

# æ ¹æ“šæ¨™é¡Œåˆ†é¡æ–°è
def classify_news(title):
    title = title.lower()
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(kw.lower() in title for kw in keywords):
            return category
    return "å…¶ä»–"

# æ“·å–æ–°èæ‘˜è¦
def extract_summary_from_link(link):
    try:
        res = requests.get(link, timeout=5)
        if res.status_code != 200:
            return None
        soup = BeautifulSoup(res.text, 'html.parser')
        full_text = soup.get_text()
        chinese_text = ''.join(c for c in full_text if '\u4e00' <= c <= '\u9fff')
        summary = chinese_text.strip().replace('\n', '').replace('\r', '')
        return summary[:50] + "..." if len(summary) >= 50 else summary
    except Exception as e:
        print(f"âš ï¸ æ‘˜è¦æ“·å–å¤±æ•—ï¼š{e}")
        return None

# æ“·å–æ–°è
def fetch_news():
    rss_urls = [
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å£½éšª+OR+é‡‘æ§+OR+äººå£½+OR+æ–°å£½+OR+å°æ–°å£½+OR+å³æ±é€²+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    ]

    classified_news = {cat: [] for cat in CATEGORY_KEYWORDS}
    processed_links = set()

    for rss_url in rss_urls:
        try:
            res = requests.get(rss_url, timeout=10)
            print(f"âœ… ä¾†æº: {rss_url} å›æ‡‰ç‹€æ…‹ï¼š{res.status_code}")
            if res.status_code != 200:
                continue
            root = ET.fromstring(res.content)
            items = root.findall(".//item")
            print(f"âœ… å¾ {rss_url} æŠ“åˆ° {len(items)} ç­†æ–°è")
        except Exception as e:
            print(f"âš ï¸ RSS æ“·å–éŒ¯èª¤ï¼š{e}")
            continue

        for item in items:
            title_elem = item.find('title')
            link_elem = item.find('link')
            pubDate_elem = item.find('pubDate')
            if title_elem is None or link_elem is None or pubDate_elem is None:
                continue

            title = title_elem.text.strip()
            link = link_elem.text.strip()
            pubDate_str = pubDate_elem.text.strip()

            if not title or title.startswith("Google ãƒ‹ãƒ¥ãƒ¼ã‚¹"):
                continue

            source_elem = item.find('source')
            source_name = source_elem.text.strip() if source_elem is not None else "æœªæ¨™ç¤º"
            pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)
            if pub_datetime.date() != today:
                continue

            if any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                continue
            if not is_taiwan_news(source_name, link):
                continue
            if link in processed_links:
                continue
            processed_links.add(link)

            summary = extract_summary_from_link(link)
            if not summary:
                continue

            short_link = shorten_url(link)
            category = classify_news(title)
            formatted = f"ğŸ“° {title}\nğŸ“Œ ä¾†æºï¼š{source_name}\nğŸ”— {short_link}\nğŸ“ {summary}"
            classified_news[category].append(formatted)

    return classified_news

# ç™¼é€ LINE å»£æ’­è¨Šæ¯
def broadcast_message(message):
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }

    data = {
        "messages": [{
            "type": "text",
            "text": message
        }]
    }

    print(f"ğŸ“¤ ç™¼é€è¨Šæ¯ç¸½é•·ï¼š{len(message)} å­—å…ƒ")
    res = requests.post(url, headers=headers, json=data)
    print(f"ğŸ“¤ LINE å›å‚³ç‹€æ…‹ç¢¼ï¼š{res.status_code}")
    print("ğŸ“¤ LINE å›å‚³å…§å®¹ï¼š", res.text)

# ç™¼é€åˆ†é¡è¨Šæ¯ + æ‘˜è¦è¨Šæ¯
def send_message_by_category(news_by_category):
    max_length = 4000
    no_news_categories = []
    all_for_summary = []

    for category in ["æ–°å…‰é‡‘æ§", "å°æ–°é‡‘æ§", "ä¿éšª", "é‡‘æ§"]:
        messages = news_by_category.get(category, [])
        if messages:
            title = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥ã€{category}ã€‘é‡é»æ–°èæ•´ç†ã€‘ å…±{len(messages)}å‰‡æ–°è"
            content = "\n\n".join(messages)
            full_message = f"{title}\n\n{content}"
            for i in range(0, len(full_message), max_length):
                broadcast_message(full_message[i:i + max_length])
            all_for_summary.extend(messages)
        else:
            no_news_categories.append(category)

    if no_news_categories:
        title = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥ç„¡ç›¸é—œæ–°èåˆ†é¡æ•´ç†ã€‘"
        content = "\n".join(f"ğŸ“‚ã€{cat}ã€‘ç„¡ç›¸é—œæ–°è" for cat in no_news_categories)
        broadcast_message(f"{title}\n\n{content}")

    # é¡å¤–æ‘˜è¦è¨Šæ¯ï¼ˆä¸å«ä¾†æºèˆ‡é€£çµï¼‰
    if all_for_summary:
        summary_lines = []
        for msg in all_for_summary:
            lines = msg.split('\n')
            title = lines[0].replace("ğŸ“° ", "").strip()
            summary = next((l.replace("ğŸ“ ", "").strip() for l in lines if l.startswith("ğŸ“ ")), "")
            summary_lines.append(f"ğŸ”¸{title}\n{summary}")
        final_message = f"ã€{today} æ¥­ä¼éƒ¨é‡é»æ‘˜è¦æ•´ç†ã€‘\n\n" + "\n\n".join(summary_lines)
        broadcast_message(final_message[:4000])

# ä¸»ç¨‹å¼
if __name__ == "__main__":
    news = fetch_news()
    if news:
        send_message_by_category(news)
    else:
        print("âš ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°èï¼Œä¸ç™¼é€ã€‚")




