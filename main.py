import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import email.utils
from urllib.parse import quote
import requests
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
import hashlib
import json
import sqlite3
from pathlib import Path
import time
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('news_bot.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# âœ… åˆå§‹åŒ–èªæ„æ¨¡å‹
try:
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    logger.info("âœ… AI æ¨¡å‹è¼‰å…¥æˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ AI æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    model = None

# âœ… å»é‡åƒæ•¸è¨­å®š
SIMILARITY_THRESHOLD = 0.82  # èªæ„ç›¸ä¼¼åº¦é–€æª»
TEXT_SIMILARITY_THRESHOLD = 0.75  # æ–‡å­—ç›¸ä¼¼åº¦é–€æª»
URL_SIMILARITY_THRESHOLD = 0.8  # URL ç›¸ä¼¼åº¦é–€æª»

ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
if ACCESS_TOKEN:
    logger.info(f"âœ… Access Token å·²è¨­å®šï¼Œå‰10ç¢¼ï¼š{ACCESS_TOKEN[:10]}")
else:
    logger.error("âŒ ACCESS_TOKEN ç’°å¢ƒè®Šæ•¸æœªè¨­å®š")

CATEGORY_KEYWORDS = {
    "æ–°å…‰é‡‘æ§": ["æ–°å…‰é‡‘", "æ–°å…‰äººå£½", "æ–°å£½", "å³æ±é€²", "æ–°å…‰é‡‘æ§"],
    "å°æ–°é‡‘æ§": ["å°æ–°é‡‘", "å°æ–°äººå£½", "å°æ–°å£½", "å³æ±äº®", "å°æ–°é‡‘æ§"],
    "é‡‘æ§": ["é‡‘æ§", "é‡‘èæ§è‚¡", "ä¸­ä¿¡é‡‘", "ç‰å±±é‡‘", "æ°¸è±é‡‘", "åœ‹æ³°é‡‘", "å¯Œé‚¦é‡‘", "å°ç£é‡‘", "ç¬¬ä¸€é‡‘", "å…ƒå¤§é‡‘"],
    "ä¿éšª": ["ä¿éšª", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª", "äººå£½", "ç”¢éšª", "è»Šéšª", "ä¿å–®"],
    "å…¶ä»–": []
}

EXCLUDED_KEYWORDS = [
    'ä¿éšªå¥—', 'é¿å­•å¥—', 'ä¿éšªå¥—ä½¿ç”¨', 'å¤ªé™½äººå£½', 'å¤§è¥¿éƒ¨äººå£½', 'ç¾åœ‹æµ·å²¸ä¿éšª',
    'é¦™æ¸¯', 'å¤§é™¸', 'ä¸­åœ‹å¤§é™¸', 'ç¾åœ‹', 'æ—¥æœ¬', 'éŸ“åœ‹', 'è¶Šå—', 'æ³°åœ‹', 'æ–°åŠ å¡'
]

TW_TZ = timezone(timedelta(hours=8))
now = datetime.now(TW_TZ)
today = now.date()

# âœ… åˆå§‹åŒ–è³‡æ–™åº«
def init_database():
    """åˆå§‹åŒ– SQLite è³‡æ–™åº«"""
    try:
        db_path = Path('news_cache.db')
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS processed_news (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                normalized_title TEXT NOT NULL,
                link TEXT NOT NULL,
                source TEXT,
                category TEXT,
                pub_date TEXT,
                processed_at TEXT NOT NULL,
                title_hash TEXT NOT NULL,
                url_hash TEXT
            )
        ''')
        
        # å»ºç«‹ç´¢å¼•æé«˜æŸ¥è©¢æ•ˆèƒ½
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_title_hash ON processed_news(title_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_url_hash ON processed_news(url_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_processed_at ON processed_news(processed_at)')
        
        # æ¸…ç†è¶…é7å¤©çš„è¨˜éŒ„
        week_ago = (now - timedelta(days=7)).isoformat()
        cursor.execute('DELETE FROM processed_news WHERE processed_at < ?', (week_ago,))
        deleted_count = cursor.rowcount
        
        conn.commit()
        logger.info(f"âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆï¼Œæ¸…ç†äº† {deleted_count} ç­†èˆŠè¨˜éŒ„")
        return conn
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

# âœ… æ”¹é€²çš„æ¨™é¡Œæ­£è¦åŒ–
def normalize_title(title):
    """æ›´åš´æ ¼çš„æ¨™é¡Œæ­£è¦åŒ–"""
    if not title:
        return ""
    
    # ç§»é™¤å¸¸è¦‹çš„æ–°èå‰ç¶´å¾Œç¶´
    title = re.sub(r'^(å¿«è¨Š|ç¨å®¶|æœ€æ–°|å³æ™‚|æ›´æ–°|åœ–è¡¨æ–°è|å°ˆé¡Œ|æ·±åº¦|åˆ†æ|é‡ç£…|çªç™¼|ç·Šæ€¥)', '', title)
    title = re.sub(r'[ï½œ|â€§\-ï¼â€“â€”~ï½].*$', '', title)  # ç§»é™¤åª’é«”å¾Œç¶´
    title = re.sub(r'<[^>]+>', '', title)            # ç§»é™¤ HTML æ¨™ç±¤
    title = re.sub(r'[ï¼ˆ()ï¼‰\[\]ã€ã€‘ã€Œã€ã€ã€""''ã€Šã€‹]', '', title)  # ç§»é™¤å„ç¨®æ‹¬è™Ÿ
    title = re.sub(r'[^\w\u4e00-\u9fff\s]', ' ', title)  # ç§»é™¤éæ–‡å­—ç¬¦è™Ÿï¼Œä¿ç•™ç©ºç™½
    title = re.sub(r'\s+', ' ', title)               # åˆä½µå¤šé¤˜ç©ºç™½
    title = re.sub(r'(è‚¡ä»½æœ‰é™å…¬å¸|æœ‰é™å…¬å¸|è‚¡ä»½|å…¬å¸|é›†åœ˜)', '', title)  # ç§»é™¤å…¬å¸å¾Œç¶´
    title = re.sub(r'(æ–°å°å¹£|å…ƒ|å„„|è¬|åƒ)', '', title)  # ç§»é™¤é‡‘é¡å–®ä½
    return title.strip().lower()

def calculate_text_similarity(text1, text2):
    """è¨ˆç®—æ–‡å­—ç›¸ä¼¼åº¦ (Jaccard similarity)"""
    if not text1 or not text2:
        return 0
    
    # åˆ†è©ä¸¦å»é™¤çŸ­è©
    words1 = set([w for w in text1.split() if len(w) >= 2])
    words2 = set([w for w in text2.split() if len(w) >= 2])
    
    if not words1 or not words2:
        return 0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    return len(intersection) / len(union) if union else 0

def calculate_url_similarity(url1, url2):
    """è¨ˆç®— URL ç›¸ä¼¼åº¦"""
    if not url1 or not url2:
        return 0
    
    # ç§»é™¤å”è­°å’Œåƒæ•¸
    def clean_url(url):
        url = re.sub(r'^https?://', '', url)
        url = re.sub(r'\?.*$', '', url)
        url = re.sub(r'#.*$', '', url)
        return url.lower()
    
    clean1 = clean_url(url1)
    clean2 = clean_url(url2)
    
    # è¨ˆç®—ç·¨è¼¯è·é›¢ç›¸ä¼¼åº¦
    from difflib import SequenceMatcher
    return SequenceMatcher(None, clean1, clean2).ratio()

def generate_hash(text):
    """ç”Ÿæˆæ–‡å­—é›œæ¹Š"""
    if not text:
        return ""
    return hashlib.md5(text.encode('utf-8')).hexdigest()[:16]

def is_duplicate_news(title, link, conn):
    """æ”¹é€²çš„é‡è¤‡æ–°èæª¢æ¸¬"""
    if not conn:
        return False
    
    try:
        normalized = normalize_title(title)
        title_hash = generate_hash(normalized)
        url_hash = generate_hash(link)
        
        cursor = conn.cursor()
        
        # 1. æª¢æŸ¥å®Œå…¨ç›¸åŒçš„æ¨™é¡Œé›œæ¹Š
        cursor.execute('SELECT title FROM processed_news WHERE title_hash = ?', (title_hash,))
        if cursor.fetchone():
            logger.info(f"ğŸ”„ ç™¼ç¾é‡è¤‡æ¨™é¡Œé›œæ¹Š: {title[:50]}...")
            return True
        
        # 2. æª¢æŸ¥ç›¸åŒçš„ URL é›œæ¹Š
        cursor.execute('SELECT title FROM processed_news WHERE url_hash = ?', (url_hash,))
        if cursor.fetchone():
            logger.info(f"ğŸ”„ ç™¼ç¾é‡è¤‡URL: {title[:50]}...")
            return True
        
        # 3. æª¢æŸ¥æ–‡å­—ç›¸ä¼¼åº¦å’Œ URL ç›¸ä¼¼åº¦
        cursor.execute('''
            SELECT normalized_title, link FROM processed_news 
            WHERE processed_at > ? 
            ORDER BY processed_at DESC 
            LIMIT 200
        ''', ((now - timedelta(hours=48)).isoformat(),))
        
        for stored_title, stored_link in cursor.fetchall():
            # æ–‡å­—ç›¸ä¼¼åº¦æª¢æŸ¥
            text_sim = calculate_text_similarity(normalized, stored_title)
            if text_sim >= TEXT_SIMILARITY_THRESHOLD:
                logger.info(f"ğŸ”„ ç™¼ç¾é«˜æ–‡å­—ç›¸ä¼¼åº¦ ({text_sim:.2f}): {title[:50]}...")
                return True
            
            # URL ç›¸ä¼¼åº¦æª¢æŸ¥
            url_sim = calculate_url_similarity(link, stored_link)
            if url_sim >= URL_SIMILARITY_THRESHOLD:
                logger.info(f"ğŸ”„ ç™¼ç¾ç›¸ä¼¼URL ({url_sim:.2f}): {title[:50]}...")
                return True
        
        return False
        
    except Exception as e:
        logger.error(f"âŒ é‡è¤‡æª¢æ¸¬å¤±æ•—: {e}")
        return False

def save_processed_news(title, link, source, category, pub_date, conn):
    """å„²å­˜å·²è™•ç†çš„æ–°è"""
    if not conn:
        return
    
    try:
        normalized = normalize_title(title)
        title_hash = generate_hash(normalized)
        url_hash = generate_hash(link)
        
        cursor = conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO processed_news 
            (id, title, normalized_title, link, source, category, pub_date, processed_at, title_hash, url_hash)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            f"{title_hash}_{int(now.timestamp())}",
            title, normalized, link, source, category, 
            pub_date, now.isoformat(), title_hash, url_hash
        ))
        conn.commit()
    except Exception as e:
        logger.error(f"âŒ å„²å­˜æ–°èå¤±æ•—: {e}")

# âœ… ä¿®æ”¹ç¶²å€è™•ç†ï¼Œé¿å… LINE é è¦½
def create_clean_url(long_url):
    """å»ºç«‹é¿å… LINE é è¦½çš„ç¶²å€"""
    try:
        # æ–¹æ³•1: ä½¿ç”¨ TinyURL
        encoded_url = quote(long_url, safe='')
        api_url = f"http://tinyurl.com/api-create.php?url={encoded_url}"
        res = requests.get(api_url, timeout=8)
        
        if res.status_code == 200 and res.text.startswith('http'):
            short_url = res.text.strip()
            # åŠ ä¸Šç ´å£é è¦½çš„åƒæ•¸
            return f"{short_url}?v={int(time.time())}"
        
    except Exception as e:
        logger.warning(f"âš ï¸ çŸ­ç¶²å€å¤±æ•—: {e}")
    
    # å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥åœ¨åŸç¶²å€åŠ åƒæ•¸
    separator = '&' if '?' in long_url else '?'
    return f"{long_url}{separator}cache_bust={int(time.time())}&noprev=1"

def classify_news(title):
    """æ–°èåˆ†é¡"""
    normalized_title = normalize_title(title)
    
    # ä¾åºæª¢æŸ¥åˆ†é¡ï¼Œå„ªå…ˆæª¢æŸ¥ç‰¹å®šå…¬å¸
    for category, keywords in CATEGORY_KEYWORDS.items():
        if category == "å…¶ä»–":  # è·³éã€Œå…¶ä»–ã€åˆ†é¡
            continue
        for keyword in keywords:
            if keyword.lower() in normalized_title:
                return category
    
    return "å…¶ä»–"

def is_taiwan_news(source_name, link, title):
    """åˆ¤æ–·æ˜¯å¦ç‚ºå°ç£æ–°è"""
    taiwan_sources = [
        'å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'ä¸‰ç«‹æ–°èç¶²', 'è‡ªç”±æ™‚å ±', 'è¯åˆæ–°èç¶²',
        'é¡é€±åˆŠ', 'å°ç£é›…è™', 'é‰…äº¨ç¶²', 'ä¸­æ™‚æ–°èç¶²', 'Ettodayæ–°èé›²', 'ETtoday',
        'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ã€Šç¾ä»£ä¿éšªã€‹é›œèªŒ', 'é è¦‹é›œèªŒ', 'å•†æ¥­å‘¨åˆŠ', 'ä»Šå‘¨åˆŠ',
        'MoneyDJ', 'é‰…äº¨', 'Anue', 'ç†è²¡ç¶²', 'ç¶“æ¿Ÿé€š', 'å¡å„ªæ–°èç¶²'
    ]
    
    # æª¢æŸ¥ä¾†æº
    if any(taiwan_source in source_name for taiwan_source in taiwan_sources):
        if "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±" not in source_name:  # æ’é™¤é¦™æ¸¯åª’é«”
            return True
    
    # æª¢æŸ¥ç¶²åŸŸ
    if '.tw' in link or 'yahoo.com' in link:
        return True
    
    # æª¢æŸ¥æ¨™é¡Œå…§å®¹ï¼ˆå°ç£ç‰¹æœ‰è©å½™ï¼‰
    taiwan_terms = ['æ–°å°å¹£', 'ç«‹æ³•é™¢', 'è¡Œæ”¿é™¢', 'é‡‘ç®¡æœƒ', 'å¤®è¡Œ', 'å°è‚¡', 'å°ç£', 'åŒ—å¸‚', 'é«˜é›„']
    if any(term in title for term in taiwan_terms):
        return True
    
    return False

def is_semantically_similar(title, processed_embeddings):
    """èªæ„ç›¸ä¼¼åº¦æª¢æ¸¬"""
    if not model or not processed_embeddings:
        return False
    
    try:
        norm_title = normalize_title(title)
        if not norm_title:
            return False
        
        current_embedding = model.encode([norm_title])
        similarities = cosine_similarity(current_embedding, processed_embeddings)[0]
        max_similarity = np.max(similarities)
        
        if max_similarity >= SIMILARITY_THRESHOLD:
            logger.info(f"ğŸ”„ ç™¼ç¾èªæ„ç›¸ä¼¼æ–°è ({max_similarity:.3f}): {title[:50]}...")
            return True
        
        return False
        
    except Exception as e:
        logger.error(f"âŒ èªæ„ç›¸ä¼¼åº¦æª¢æ¸¬å¤±æ•—: {e}")
        return False

def fetch_news():
    """ä¸»è¦æ–°èæŠ“å–å‡½æ•¸"""
    logger.info("ğŸš€ é–‹å§‹æŠ“å–æ–°è...")
    
    conn = init_database()
    if not conn:
        logger.error("âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
        return {}
    
    rss_urls = [
        "https://news.google.com/rss/search?q=(æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å£½éšª+OR+é‡‘æ§+OR+äººå£½)+when:1d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²+when:1d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®+when:1d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=(å£½éšª+OR+å¥åº·éšª+OR+æ„å¤–éšª+OR+äººå£½)+when:1d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=(é‡‘æ§+OR+é‡‘èæ§è‚¡)+when:1d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    ]

    classified_news = {cat: [] for cat in CATEGORY_KEYWORDS}
    processed_embeddings = []
    stats = {
        'total_fetched': 0,
        'processed': 0,
        'duplicates': 0,
        'filtered_out': 0
    }

    # è¼‰å…¥æœ€è¿‘çš„æ–°èæ¨™é¡Œé€²è¡Œæ¯”è¼ƒ
    try:
        cursor = conn.cursor()
        cursor.execute('''
            SELECT normalized_title FROM processed_news 
            WHERE processed_at > ? 
            ORDER BY processed_at DESC 
            LIMIT 300
        ''', ((now - timedelta(hours=48)).isoformat(),))
        
        recent_titles = [row[0] for row in cursor.fetchall()]
        
        if recent_titles and model:
            logger.info(f"âœ… è¼‰å…¥ {len(recent_titles)} å€‹æœ€è¿‘æ¨™é¡Œç”¨æ–¼èªæ„æ¯”è¼ƒ")
            processed_embeddings = model.encode(recent_titles)
            
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥æ­·å²æ¨™é¡Œå¤±æ•—: {e}")

    # è™•ç†æ¯å€‹ RSS ä¾†æº
    for i, rss_url in enumerate(rss_urls, 1):
        try:
            logger.info(f"ğŸ“¡ è™•ç† RSS ä¾†æº {i}/{len(rss_urls)}")
            
            res = requests.get(rss_url, timeout=15, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            if res.status_code != 200:
                logger.warning(f"âš ï¸ RSS å›æ‡‰ç•°å¸¸: {res.status_code}")
                continue

            root = ET.fromstring(res.content)
            items = root.findall(".//item")
            stats['total_fetched'] += len(items)
            
            logger.info(f"âœ… æŠ“åˆ° {len(items)} ç­†æ–°è")

            for item in items:
                try:
                    # è§£ææ–°èé …ç›®
                    title_elem = item.find('title')
                    link_elem = item.find('link')
                    pubDate_elem = item.find('pubDate')
                    
                    if not all([title_elem, link_elem, pubDate_elem]):
                        continue

                    title = title_elem.text.strip()
                    link = link_elem.text.strip()
                    pubDate_str = pubDate_elem.text.strip()
                    
                    if not title or title.startswith("Google ãƒ‹ãƒ¥ãƒ¼ã‚¹"):
                        continue

                    source_elem = item.find('source')
                    source_name = source_elem.text.strip() if source_elem is not None else "æœªæ¨™ç¤º"
                    
                    # è§£æç™¼å¸ƒæ™‚é–“
                    try:
                        pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)
                    except:
                        continue

                    # åŸºæœ¬éæ¿¾æ¢ä»¶
                    if now - pub_datetime > timedelta(hours=24):
                        stats['filtered_out'] += 1
                        continue
                        
                    if any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                        stats['filtered_out'] += 1
                        continue
                        
                    if not is_taiwan_news(source_name, link, title):
                        stats['filtered_out'] += 1
                        continue

                    # é‡è¤‡æª¢æ¸¬
                    if is_duplicate_news(title, link, conn):
                        stats['duplicates'] += 1
                        continue
                    
                    if is_semantically_similar(title, processed_embeddings):
                        stats['duplicates'] += 1
                        continue

                    # æ–°èè™•ç†
                    clean_url = create_clean_url(link)
                    category = classify_news(title)
                    
                    # æ ¼å¼åŒ–æ–°èï¼ˆé¿å… LINE é è¦½ï¼‰
                    formatted = f"ğŸ“° {title}\nğŸ“Œ {source_name}\nğŸ”— è©³æƒ…: {clean_url}"
                    classified_news[category].append(formatted)
                    
                    # å„²å­˜åˆ°è³‡æ–™åº«
                    save_processed_news(title, link, source_name, category, 
                                      pub_datetime.isoformat(), conn)
                    
                    # æ›´æ–°èªæ„æ¯”è¼ƒåŸºæº–
                    if model and len(processed_embeddings) > 0:
                        try:
                            new_embedding = model.encode([normalize_title(title)])
                            processed_embeddings = np.vstack([processed_embeddings, new_embedding])
                        except:
                            pass
                    
                    stats['processed'] += 1
                    
                except Exception as e:
                    logger.error(f"âŒ è™•ç†å–®ç­†æ–°èå¤±æ•—: {e}")
                    continue

        except Exception as e:
            logger.error(f"âŒ è™•ç† RSS ä¾†æºå¤±æ•—: {e}")
            continue

    conn.close()
    
    # è¼¸å‡ºçµ±è¨ˆè³‡è¨Š
    logger.info(f"""
ğŸ“Š æ–°èè™•ç†çµ±è¨ˆ:
   ç¸½æŠ“å–: {stats['total_fetched']} ç­†
   æˆåŠŸè™•ç†: {stats['processed']} ç­†
   é‡è¤‡éæ¿¾: {stats['duplicates']} ç­†
   å…¶ä»–éæ¿¾: {stats['filtered_out']} ç­†
""")
    
    return classified_news

def send_message_by_category(news_by_category):
    """åˆ†é¡ç™¼é€ LINE è¨Šæ¯"""
    if not ACCESS_TOKEN:
        logger.error("âŒ ACCESS_TOKEN æœªè¨­å®šï¼Œç„¡æ³•ç™¼é€è¨Šæ¯")
        return
    
    max_length = 3800  # LINE è¨Šæ¯é•·åº¦é™åˆ¶
    successful_sends = 0
    
    for category, messages in news_by_category.items():
        if not messages:
            continue
            
        try:
            # å»ºç«‹åˆ†é¡æ¨™é¡Œ
            title = f"ã€{today} {category} æ–°èã€‘å…± {len(messages)} å‰‡"
            divider = "=" * 35
            content = f"\n{divider}\n".join(messages)
            full_message = f"{title}\n{divider}\n{content}"
            
            # è™•ç†è¶…é•·è¨Šæ¯
            if len(full_message) <= max_length:
                if broadcast_message(full_message):
                    successful_sends += 1
            else:
                # åˆ†æ®µç™¼é€
                segments = []
                current_segment = f"{title}\n{divider}\n"
                
                for msg in messages:
                    if len(current_segment + msg + f"\n{divider}\n") <= max_length:
                        current_segment += msg + f"\n{divider}\n"
                    else:
                        segments.append(current_segment.rstrip(f"\n{divider}\n"))
                        current_segment = f"{title} (çºŒ)\n{divider}\n{msg}\n{divider}\n"
                
                if current_segment.strip():
                    segments.append(current_segment.rstrip(f"\n{divider}\n"))
                
                for i, segment in enumerate(segments):
                    if broadcast_message(segment):
                        successful_sends += 1
                    time.sleep(1)  # é¿å…ç™¼é€éå¿«
                        
        except Exception as e:
            logger.error(f"âŒ ç™¼é€ {category} åˆ†é¡å¤±æ•—: {e}")

    # ç™¼é€ç„¡æ–°èé€šçŸ¥
    empty_categories = [cat for cat, msgs in news_by_category.items() if not msgs]
    if empty_categories:
        title = f"ã€{today} ç„¡æ–°èåˆ†é¡ã€‘"
        content = "\n".join(f"ğŸ“‚ {cat}: ç„¡ç›¸é—œæ–°è" for cat in empty_categories)
        broadcast_message(f"{title}\n{content}")
        
    logger.info(f"âœ… æˆåŠŸç™¼é€ {successful_sends} å€‹åˆ†é¡çš„è¨Šæ¯")

def broadcast_message(message):
    """ç™¼é€ LINE å»£æ’­è¨Šæ¯"""
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }

    data = {
        "messages": [{
            "type": "text",
            "text": message
        }]
    }

    try:
        logger.info(f"ğŸ“¤ ç™¼é€è¨Šæ¯ ({len(message)} å­—å…ƒ)")
        res = requests.post(url, headers=headers, json=data, timeout=15)
        
        if res.status_code == 200:
            logger.info("âœ… è¨Šæ¯ç™¼é€æˆåŠŸ")
            return True
        else:
            logger.error(f"âŒ ç™¼é€å¤±æ•— - ç‹€æ…‹ç¢¼: {res.status_code}, å›æ‡‰: {res.text}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ç™¼é€è¨Šæ¯ç•°å¸¸: {e}")
        return False

if __name__ == "__main__":
    start_time = time.time()
    logger.info(f"ğŸš€ æ–°èçˆ¬å–ç³»çµ±å•Ÿå‹• - {now}")
    
    try:
        news = fetch_news()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°èéœ€è¦ç™¼é€
        total_news = sum(len(msgs) for msgs in news.values())
        
        if total_news > 0:
            logger.info(f"ğŸ“¨ æº–å‚™ç™¼é€ {total_news} å‰‡æ–°è")
            send_message_by_category(news)
        else:
            logger.info("â„¹ï¸ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ–°è")
            # ç™¼é€ç„¡æ–°èé€šçŸ¥
            broadcast_message(f"ã€{today} æ¥­ä¼éƒ¨æ–°èã€‘\nä»Šæ—¥æš«ç„¡ç¬¦åˆæ¢ä»¶çš„é‡é»æ–°è")
        
        elapsed = time.time() - start_time
        logger.info(f"âœ… ç³»çµ±åŸ·è¡Œå®Œæˆï¼Œè€—æ™‚ {elapsed:.1f} ç§’")
        
    except Exception as e:
        logger.error(f"âŒ ç³»çµ±åŸ·è¡Œå¤±æ•—: {e}")
        # ç™¼é€éŒ¯èª¤é€šçŸ¥
        if ACCESS_TOKEN:
            broadcast_message(f"ã€ç³»çµ±é€šçŸ¥ã€‘\næ–°èçˆ¬å–ç³»çµ±åŸ·è¡Œç•°å¸¸: {str(e)[:100]}")



